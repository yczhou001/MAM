# MAM: ModularMulti-Agent Framework for Multi-Modal Medical Diagnosis via Role-Specialized Collaboration

[ðŸ“¢ [paper](https://github.com/yczhou001/MAM)] [[inference](https://github.com/yczhou001/MAM)] [[Dataset](https://github.com/yczhou001/MAM)] [[Models](https://github.com/yczhou001/MAM)]

<p align="center"> <img src="images/pipeline.jpg" width="100%"> </p>

## ðŸ“° News
* **[2025.00.00]** 
* **[2025.00.00]** 
* **[2025.00.00]** 
* **[2025.05.16]** [MAM](https://github.com/yczhou001/MAM) is accepted as ACL 2025 (findings).

## Abstract

Recent advancements in medical Large Language Models (LLMs) have showcased their powerful reasoning and diagnostic capabilities. Despite their success, current unified multimodal medical LLMs face limitations in knowledge update costs, comprehensiveness, and flexibility. To address these challenges, we introduce the Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis (MAM). Inspired by our empirical findings highlighting the benefits of role assignment and diagnostic discernment in LLMs, MAM decomposes the medical diagnostic process into specialized roles: a General Practitioner, Specialist Team, Radiologist, Medical Assistant, and Director, each embodied by an LLM-based agent. This modular and collaborative framework enables efficient knowledge updates and leverages existing medical LLMs and knowledge bases. Extensive experimental evaluations conducted on a wide range of publicly accessible multimodal medical datasets, incorporating text, image, audio, and video modalities, demonstrate that MAM consistently surpasses the performance of modality-specific LLMs. Notably, MAM achieves significant performance improvements ranging from 18% to 365% compared to baseline models. For more details, please refer to the [paper](https://github.com/yczhou001/MAM).
